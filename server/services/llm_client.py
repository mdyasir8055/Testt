"""
LLM client implementations for various providers (Groq, Google Gemini, Hugging Face).
"""
import os
import json
from typing import Dict, List, Optional, Any
from abc import ABC, abstractmethod
import requests
import time

class LLMClient(ABC):
    """Abstract base class for LLM clients."""
    
    @abstractmethod
    def generate(self, prompt: str, max_tokens: int = 500, temperature: float = 0.7, **kwargs) -> str:
        """Generate text response from prompt."""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """Check if the LLM service is available."""
        pass

class GroqClient(LLMClient):
    """Client for Groq API."""
    
    def __init__(self, api_key: Optional[str] = None, model: str = "mixtral-8x7b-32768"):
        self.api_key = api_key or os.getenv("GROQ_API_KEY")
        self.model = model
        self.base_url = "https://api.groq.com/openai/v1"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
    
    def generate(self, prompt: str, max_tokens: int = 500, temperature: float = 0.7, **kwargs) -> str:
        """Generate text using Groq API."""
        if not self.api_key:
            raise ValueError("Groq API key not provided")
        
        payload = {
            "model": self.model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": False
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=self.headers,
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            
            data = response.json()
            return data["choices"][0]["message"]["content"].strip()
            
        except requests.exceptions.RequestException as e:
            raise Exception(f"Groq API request failed: {str(e)}")
        except (KeyError, IndexError) as e:
            raise Exception(f"Unexpected Groq API response format: {str(e)}")
    
    def is_available(self) -> bool:
        """Check if Groq API is available."""
        if not self.api_key:
            return False
        
        try:
            response = requests.get(
                f"{self.base_url}/models",
                headers=self.headers,
                timeout=10
            )
            return response.status_code == 200
        except:
            return False

class GeminiClient(LLMClient):
    """Client for Google Gemini API."""
    
    def __init__(self, api_key: Optional[str] = None, model: str = "gemini-pro"):
        self.api_key = api_key or os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        self.model = model
        self.base_url = "https://generativelanguage.googleapis.com/v1beta"
    
    def generate(self, prompt: str, max_tokens: int = 500, temperature: float = 0.7, **kwargs) -> str:
        """Generate text using Gemini API."""
        if not self.api_key:
            raise ValueError("Gemini API key not provided")
        
        url = f"{self.base_url}/models/{self.model}:generateContent"
        
        payload = {
            "contents": [
                {
                    "parts": [
                        {"text": prompt}
                    ]
                }
            ],
            "generationConfig": {
                "maxOutputTokens": max_tokens,
                "temperature": temperature,
                "candidateCount": 1
            }
        }
        
        try:
            response = requests.post(
                url,
                params={"key": self.api_key},
                headers={"Content-Type": "application/json"},
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            
            data = response.json()
            if "candidates" in data and len(data["candidates"]) > 0:
                content = data["candidates"][0]["content"]["parts"][0]["text"]
                return content.strip()
            else:
                raise Exception("No response generated by Gemini")
                
        except requests.exceptions.RequestException as e:
            raise Exception(f"Gemini API request failed: {str(e)}")
        except (KeyError, IndexError) as e:
            raise Exception(f"Unexpected Gemini API response format: {str(e)}")
    
    def is_available(self) -> bool:
        """Check if Gemini API is available."""
        if not self.api_key:
            return False
        
        try:
            url = f"{self.base_url}/models"
            response = requests.get(
                url,
                params={"key": self.api_key},
                timeout=10
            )
            return response.status_code == 200
        except:
            return False

class HuggingFaceClient(LLMClient):
    """Client for Hugging Face Inference API."""
    
    def __init__(self, api_key: Optional[str] = None, model: str = "microsoft/DialoGPT-large"):
        self.api_key = api_key or os.getenv("HUGGINGFACE_API_KEY") or os.getenv("HF_API_KEY")
        self.model = model
        self.base_url = "https://api-inference.huggingface.co/models"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
    
    def generate(self, prompt: str, max_tokens: int = 500, temperature: float = 0.7, **kwargs) -> str:
        """Generate text using Hugging Face API."""
        if not self.api_key:
            raise ValueError("Hugging Face API key not provided")
        
        # For text generation models
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": max_tokens,
                "temperature": temperature,
                "return_full_text": False,
                "do_sample": True
            }
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/{self.model}",
                headers=self.headers,
                json=payload,
                timeout=30
            )
            
            # Handle model loading
            if response.status_code == 503:
                data = response.json()
                if "loading" in data.get("error", "").lower():
                    # Wait for model to load and retry
                    time.sleep(10)
                    response = requests.post(
                        f"{self.base_url}/{self.model}",
                        headers=self.headers,
                        json=payload,
                        timeout=30
                    )
            
            response.raise_for_status()
            data = response.json()
            
            if isinstance(data, list) and len(data) > 0:
                return data[0].get("generated_text", "").strip()
            else:
                raise Exception("Unexpected response format from Hugging Face")
                
        except requests.exceptions.RequestException as e:
            raise Exception(f"Hugging Face API request failed: {str(e)}")
        except (KeyError, IndexError) as e:
            raise Exception(f"Unexpected Hugging Face API response format: {str(e)}")
    
    def is_available(self) -> bool:
        """Check if Hugging Face API is available."""
        if not self.api_key:
            return False
        
        try:
            # Test with a simple request
            payload = {"inputs": "Hello"}
            response = requests.post(
                f"{self.base_url}/{self.model}",
                headers=self.headers,
                json=payload,
                timeout=10
            )
            return response.status_code in [200, 503]  # 503 is model loading
        except:
            return False

class LocalLLMClient(LLMClient):
    """Client for local LLM models (fallback option)."""
    
    def __init__(self, model_name: str = "local"):
        self.model_name = model_name
    
    def generate(self, prompt: str, max_tokens: int = 500, temperature: float = 0.7, **kwargs) -> str:
        """Generate text using local model (placeholder implementation)."""
        # This would integrate with local models like Ollama, transformers, etc.
        # For now, return a helpful message
        return (
            "I'm a local language model assistant. To provide better responses, "
            "please configure an API key for Groq, Google Gemini, or Hugging Face "
            "in your environment variables."
        )
    
    def is_available(self) -> bool:
        """Local model is always available as fallback."""
        return True

class LLMFactory:
    """Factory class for creating LLM clients."""
    
    @staticmethod
    def create_client(provider: str = "groq", **kwargs) -> LLMClient:
        """
        Create an LLM client based on provider.
        
        Args:
            provider: LLM provider name (groq, gemini, huggingface, local)
            **kwargs: Additional arguments for the client
            
        Returns:
            LLMClient instance
        """
        providers = {
            "groq": GroqClient,
            "gemini": GeminiClient,
            "google": GeminiClient,  # Alias
            "huggingface": HuggingFaceClient,
            "hf": HuggingFaceClient,  # Alias
            "local": LocalLLMClient
        }
        
        provider_class = providers.get(provider.lower())
        if not provider_class:
            raise ValueError(f"Unknown provider: {provider}")
        
        return provider_class(**kwargs)
    
    @staticmethod
    def get_available_providers() -> List[str]:
        """Get list of available providers with working API keys."""
        available = []
        
        providers = ["groq", "gemini", "huggingface"]
        for provider in providers:
            try:
                client = LLMFactory.create_client(provider)
                if client.is_available():
                    available.append(provider)
            except:
                continue
        
        # Local is always available as fallback
        available.append("local")
        
        return available

# Usage example and testing
if __name__ == "__main__":
    # Test available providers
    available = LLMFactory.get_available_providers()
    print(f"Available providers: {available}")
    
    # Test with first available provider
    if available:
        provider = available[0]
        print(f"Testing {provider} provider...")
        
        try:
            client = LLMFactory.create_client(provider)
            response = client.generate(
                "What is artificial intelligence? Keep the response brief.",
                max_tokens=100
            )
            print(f"Response: {response}")
        except Exception as e:
            print(f"Error testing {provider}: {e}")
